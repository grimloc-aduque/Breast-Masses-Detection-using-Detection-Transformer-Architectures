{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from detr_config import Config\n",
    "from detr_dataset import get_dataset, get_dataloader\n",
    "from detr_trainer import get_trainer\n",
    "from detr_evaluation import get_metrics\n",
    "from detr_model import DETRModel\n",
    "from transformers import (DeformableDetrConfig,\n",
    "                          DeformableDetrForObjectDetection,\n",
    "                          DeformableDetrImageProcessor, DetrConfig,\n",
    "                          DetrForObjectDetection, DetrImageProcessor)\n",
    "\n",
    "STDOUT = sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------- MODEL -------- model=DETR_backbone=resnet50_queries=50_dmodel=256_layers=6 \n",
      "------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\detr-env\\Lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:197: UserWarning: Attribute 'detr_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['detr_model'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\ProgramData\\miniconda3\\envs\\detr-env\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:617: UserWarning: Checkpoint directory ./lightning_logs\\model=DETR_backbone=resnet50_queries=50_dmodel=256_layers=6\\fold_1\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name       | Type                   | Params\n",
      "------------------------------------------------------\n",
      "0 | detr_model | DetrForObjectDetection | 41.5 M\n",
      "------------------------------------------------------\n",
      "41.3 M    Trainable params\n",
      "222 K     Non-trainable params\n",
      "41.5 M    Total params\n",
      "165.955   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.named_parameters at 0x000002B7A04C7640>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091ed9d8b38c46c88d39dab88702be26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\detr-env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:490: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "c:\\ProgramData\\miniconda3\\envs\\detr-env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "c:\\ProgramData\\miniconda3\\envs\\detr-env\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "c:\\ProgramData\\miniconda3\\envs\\detr-env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0116eaac0c04546880c6e9e0c6ba291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\detr-env\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "c:\\ProgramData\\miniconda3\\envs\\detr-env\\Lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:197: UserWarning: Attribute 'detr_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['detr_model'])`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DETRModel.forward() missing 1 required positional argument: 'pixel_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Alejandro Duque\\Documents\\USFQ\\Proyecto Integrador\\Workspace\\DETR\\nb04_train_validate_DETR.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alejandro%20Duque/Documents/USFQ/Proyecto%20Integrador/Workspace/DETR/nb04_train_validate_DETR.ipynb#X13sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m checkpoint_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(checkpoints_dir, best_checkpoint)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alejandro%20Duque/Documents/USFQ/Proyecto%20Integrador/Workspace/DETR/nb04_train_validate_DETR.ipynb#X13sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m model \u001b[39m=\u001b[39m DETRModel\u001b[39m.\u001b[39mload_from_checkpoint(checkpoint_path, map_location\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Alejandro%20Duque/Documents/USFQ/Proyecto%20Integrador/Workspace/DETR/nb04_train_validate_DETR.ipynb#X13sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m metrics_dict \u001b[39m=\u001b[39m get_metrics(model, valid_dataset, image_processor, threshold\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Alejandro%20Duque/Documents/USFQ/Proyecto%20Integrador/Workspace/DETR/nb04_train_validate_DETR.ipynb#X13sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m metrics_by_fold\u001b[39m.\u001b[39mappend(metrics_dict)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Alejandro%20Duque/Documents/USFQ/Proyecto%20Integrador/Workspace/DETR/nb04_train_validate_DETR.ipynb#X13sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m index\u001b[39m.\u001b[39mappend(fold_name)\n",
      "File \u001b[1;32mc:\\Users\\Alejandro Duque\\Documents\\USFQ\\Proyecto Integrador\\Workspace\\DETR\\detr_evaluation.py:57\u001b[0m, in \u001b[0;36mget_metrics\u001b[1;34m(model, dataset, image_processor, threshold)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m     56\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 57\u001b[0m         outputs \u001b[39m=\u001b[39m model(\n\u001b[0;32m     58\u001b[0m             pixel_values \u001b[39m=\u001b[39;49m batch[\u001b[39m\"\u001b[39;49m\u001b[39mpixel_values\u001b[39;49m\u001b[39m\"\u001b[39;49m], \n\u001b[0;32m     59\u001b[0m             \u001b[39m# pixel_mask =  batch[\"pixel_mask\"],\u001b[39;49;00m\n\u001b[0;32m     60\u001b[0m         )\n\u001b[0;32m     61\u001b[0m     labels \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     62\u001b[0m     orig_target_sizes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([target[\u001b[39m\"\u001b[39m\u001b[39morig_size\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m target \u001b[39min\u001b[39;00m labels], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)        \n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\detr-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: DETRModel.forward() missing 1 required positional argument: 'pixel_mask'"
     ]
    }
   ],
   "source": [
    "\n",
    "# HyperParameters\n",
    "\n",
    "hyperparameters = Config.HYPERPARAMS\n",
    "\n",
    "hyperparameters = [\n",
    "    ('DETR', 'resnet50', 50, 256, 6),\n",
    "    # ('D-DETR', 'resnet50', 300, 256, 6),\n",
    "]\n",
    "\n",
    "# Hyperparameter Search\n",
    "\n",
    "for architecture, backbone, num_queries, d_model, transformer_layers in hyperparameters:\n",
    "    \n",
    "    if architecture == 'DETR':\n",
    "        IMG_PROCESSOR_CLASS = DetrImageProcessor\n",
    "        DETR_CONFIG_CLASS = DetrConfig\n",
    "        DETR_CLASS = DetrForObjectDetection\n",
    "    else:\n",
    "        IMG_PROCESSOR_CLASS = DeformableDetrImageProcessor\n",
    "        DETR_CONFIG_CLASS = DeformableDetrConfig\n",
    "        DETR_CLASS = DeformableDetrForObjectDetection    \n",
    "    \n",
    "    # Model Configuration\n",
    "\n",
    "    config = DETR_CONFIG_CLASS(\n",
    "        num_labels = Config.NUM_CLASSES,\n",
    "        id2label = {0:'Mass'}, \n",
    "        label2id = {'Mass': 0},\n",
    "        num_queries = num_queries,\n",
    "        d_model = d_model,\n",
    "        encoder_layers = transformer_layers,\n",
    "        decoder_layers = transformer_layers,\n",
    "        backbone=backbone\n",
    "    )\n",
    "    \n",
    "    # Model Directory\n",
    "\n",
    "    model_name = [\n",
    "        f'model={architecture}',\n",
    "        f'backbone={backbone.split(\".\")[0]}',\n",
    "        f'queries={num_queries}',\n",
    "        f'dmodel={d_model}',\n",
    "        f'layers={transformer_layers}'\n",
    "    ]\n",
    "    \n",
    "    model_name = '_'.join(model_name)\n",
    "    model_dir = os.path.join(Config.LOGS_DIR, model_name)\n",
    "    # if os.path.exists(model_dir):\n",
    "    #     shutil.rmtree(model_dir)\n",
    "    print('\\n-------- MODEL --------', model_name,\n",
    "          '\\n------------------------\\n')\n",
    "    \n",
    "\n",
    "    # K-fold Cross Validation \n",
    "    \n",
    "    metrics_by_fold = []\n",
    "    index = []\n",
    "\n",
    "    for fold in range(1,11):\n",
    "        \n",
    "        # Model\n",
    "        \n",
    "        image_processor = IMG_PROCESSOR_CLASS()        \n",
    "        detr_model = DETR_CLASS(config=config)\n",
    "        model = DETRModel(detr_model=detr_model)\n",
    "        \n",
    "        # Datasets\n",
    "        \n",
    "        fold_name = f'fold_{fold}'\n",
    "        dataset_dir = os.path.join(Config.DATASET, fold_name)\n",
    "        \n",
    "        train_dir = os.path.join(dataset_dir, 'train')\n",
    "        train_dataset = get_dataset(train_dir, image_processor)\n",
    "        train_loader = get_dataloader(train_dataset, image_processor)\n",
    "        \n",
    "        valid_dir = os.path.join(dataset_dir, 'valid')\n",
    "        valid_dataset = get_dataset(valid_dir, image_processor)\n",
    "        valid_loader = get_dataloader(valid_dataset, image_processor)\n",
    "\n",
    "        # Training\n",
    "        \n",
    "        version = os.path.join(model_name, fold_name)\n",
    "        trainer = get_trainer(version)\n",
    "        trainer.fit(model, train_dataloaders=train_loader,\n",
    "                    val_dataloaders=valid_loader)\n",
    "        \n",
    "        # Validation\n",
    "        \n",
    "        checkpoints_dir = os.path.join(Config.LOGS_DIR, version, 'checkpoints')\n",
    "        best_checkpoint = [f for f in os.listdir(checkpoints_dir) if 'last' not in f][0]\n",
    "        checkpoint_path = os.path.join(checkpoints_dir, best_checkpoint)\n",
    "        model = DETRModel.load_from_checkpoint(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "        metrics_dict = get_metrics(model, valid_dataset, image_processor, threshold=0.001)\n",
    "        metrics_by_fold.append(metrics_dict)\n",
    "        index.append(fold_name)\n",
    "        \n",
    "        if fold != 1:\n",
    "            print(\"Cleaning Checkpoints\")\n",
    "            # shutil.rmtree(checkpoints_dir)\n",
    "\n",
    "        break # Fold\n",
    "    \n",
    "    # Aggregate Metrics\n",
    "    \n",
    "    metrics_by_fold = pd.DataFrame(metrics_by_fold, index=index)\n",
    "    metrics_by_fold.loc['mean'] = metrics_by_fold.mean()\n",
    "    metrics_path = os.path.join(Config.LOGS_DIR, model_name, Config.METRICS_FILE)\n",
    "    metrics_by_fold.to_csv(metrics_path)\n",
    "    \n",
    "    break # Hyperparameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
