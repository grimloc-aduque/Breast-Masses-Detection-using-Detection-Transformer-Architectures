{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from detr_config import Config\n",
    "from detr_factory import DETRFactory\n",
    "from transformers import (DeformableDetrConfig,\n",
    "                          DeformableDetrForObjectDetection,\n",
    "                          DeformableDetrImageProcessor, DetrConfig,\n",
    "                          DetrForObjectDetection, DetrImageProcessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    num_model_params = sum(p.numel() for p in model.parameters()) / (1e6)\n",
    "\n",
    "    backbone_params = [param for name, param in model.named_parameters() if \"backbone\" in name]\n",
    "    num_backbone_params = sum(p.numel() for p in backbone_params) / (1e6)\n",
    "\n",
    "    transformer_params =  [param for name, param in model.named_parameters() if \"backbone\" not in name]\n",
    "    num_transformer_params = sum(p.numel() for p in transformer_params) / (1e6)\n",
    "\n",
    "    print(f\"{model.config.backbone}\",\n",
    "          f'-> Model: {round(num_model_params,1)} M',\n",
    "          f'- Backbone: {round(num_backbone_params,1)} M',\n",
    "          f'- Transformer: {round(num_transformer_params,1)} M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeformableDetrForObjectDetection were not initialized from the model checkpoint at SenseTime/deformable-detr and are newly initialized because the shapes did not match:\n",
      "- class_embed.0.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- class_embed.0.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer1.0.conv1.weight: found shape torch.Size([64, 64, 1, 1]) in the checkpoint and torch.Size([64, 64, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer1.1.conv1.weight: found shape torch.Size([64, 256, 1, 1]) in the checkpoint and torch.Size([64, 64, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer2.0.conv1.weight: found shape torch.Size([128, 256, 1, 1]) in the checkpoint and torch.Size([128, 64, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer2.0.downsample.0.weight: found shape torch.Size([512, 256, 1, 1]) in the checkpoint and torch.Size([128, 64, 1, 1]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer2.0.downsample.1.bias: found shape torch.Size([512]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer2.0.downsample.1.running_mean: found shape torch.Size([512]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer2.0.downsample.1.running_var: found shape torch.Size([512]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer2.0.downsample.1.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer2.1.conv1.weight: found shape torch.Size([128, 512, 1, 1]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.0.conv1.weight: found shape torch.Size([256, 512, 1, 1]) in the checkpoint and torch.Size([256, 128, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.0.downsample.0.weight: found shape torch.Size([1024, 512, 1, 1]) in the checkpoint and torch.Size([256, 128, 1, 1]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.0.downsample.1.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.0.downsample.1.running_mean: found shape torch.Size([1024]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.0.downsample.1.running_var: found shape torch.Size([1024]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.0.downsample.1.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.1.conv1.weight: found shape torch.Size([256, 1024, 1, 1]) in the checkpoint and torch.Size([256, 256, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer4.0.conv1.weight: found shape torch.Size([512, 1024, 1, 1]) in the checkpoint and torch.Size([512, 256, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer4.0.downsample.0.weight: found shape torch.Size([2048, 1024, 1, 1]) in the checkpoint and torch.Size([512, 256, 1, 1]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer4.0.downsample.1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer4.0.downsample.1.running_mean: found shape torch.Size([2048]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer4.0.downsample.1.running_var: found shape torch.Size([2048]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer4.0.downsample.1.weight: found shape torch.Size([2048]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer4.1.conv1.weight: found shape torch.Size([512, 2048, 1, 1]) in the checkpoint and torch.Size([512, 512, 3, 3]) in the model instantiated\n",
      "- model.input_proj.0.0.weight: found shape torch.Size([256, 512, 1, 1]) in the checkpoint and torch.Size([256, 128, 1, 1]) in the model instantiated\n",
      "- model.input_proj.1.0.weight: found shape torch.Size([256, 1024, 1, 1]) in the checkpoint and torch.Size([256, 256, 1, 1]) in the model instantiated\n",
      "- model.input_proj.2.0.weight: found shape torch.Size([256, 2048, 1, 1]) in the checkpoint and torch.Size([256, 512, 1, 1]) in the model instantiated\n",
      "- model.input_proj.3.0.weight: found shape torch.Size([256, 2048, 3, 3]) in the checkpoint and torch.Size([256, 512, 3, 3]) in the model instantiated\n",
      "- model.query_position_embeddings.weight: found shape torch.Size([300, 512]) in the checkpoint and torch.Size([100, 512]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DeformableDetrForObjectDetection were not initialized from the model checkpoint at SenseTime/deformable-detr and are newly initialized because the shapes did not match:\n",
      "- class_embed.0.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- class_embed.0.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated\n",
      "- model.query_position_embeddings.weight: found shape torch.Size([300, 512]) in the checkpoint and torch.Size([100, 512]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DeformableDetrForObjectDetection were not initialized from the model checkpoint at SenseTime/deformable-detr and are newly initialized because the shapes did not match:\n",
      "- class_embed.0.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- class_embed.0.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer1.0.conv1.weight: found shape torch.Size([64, 64, 1, 1]) in the checkpoint and torch.Size([64, 64, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer1.1.conv1.weight: found shape torch.Size([64, 256, 1, 1]) in the checkpoint and torch.Size([64, 64, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer1.2.conv1.weight: found shape torch.Size([64, 256, 1, 1]) in the checkpoint and torch.Size([64, 64, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer2.0.conv1.weight: found shape torch.Size([128, 256, 1, 1]) in the checkpoint and torch.Size([128, 64, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer2.0.downsample.0.weight: found shape torch.Size([512, 256, 1, 1]) in the checkpoint and torch.Size([128, 64, 1, 1]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer2.0.downsample.1.bias: found shape torch.Size([512]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer2.0.downsample.1.running_mean: found shape torch.Size([512]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer2.0.downsample.1.running_var: found shape torch.Size([512]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer2.0.downsample.1.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer2.1.conv1.weight: found shape torch.Size([128, 512, 1, 1]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer2.2.conv1.weight: found shape torch.Size([128, 512, 1, 1]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer2.3.conv1.weight: found shape torch.Size([128, 512, 1, 1]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.0.conv1.weight: found shape torch.Size([256, 512, 1, 1]) in the checkpoint and torch.Size([256, 128, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.0.downsample.0.weight: found shape torch.Size([1024, 512, 1, 1]) in the checkpoint and torch.Size([256, 128, 1, 1]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.0.downsample.1.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.0.downsample.1.running_mean: found shape torch.Size([1024]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.0.downsample.1.running_var: found shape torch.Size([1024]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.0.downsample.1.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.1.conv1.weight: found shape torch.Size([256, 1024, 1, 1]) in the checkpoint and torch.Size([256, 256, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.2.conv1.weight: found shape torch.Size([256, 1024, 1, 1]) in the checkpoint and torch.Size([256, 256, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.3.conv1.weight: found shape torch.Size([256, 1024, 1, 1]) in the checkpoint and torch.Size([256, 256, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.4.conv1.weight: found shape torch.Size([256, 1024, 1, 1]) in the checkpoint and torch.Size([256, 256, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer3.5.conv1.weight: found shape torch.Size([256, 1024, 1, 1]) in the checkpoint and torch.Size([256, 256, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer4.0.conv1.weight: found shape torch.Size([512, 1024, 1, 1]) in the checkpoint and torch.Size([512, 256, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer4.0.downsample.0.weight: found shape torch.Size([2048, 1024, 1, 1]) in the checkpoint and torch.Size([512, 256, 1, 1]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer4.0.downsample.1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer4.0.downsample.1.running_mean: found shape torch.Size([2048]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer4.0.downsample.1.running_var: found shape torch.Size([2048]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer4.0.downsample.1.weight: found shape torch.Size([2048]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer4.1.conv1.weight: found shape torch.Size([512, 2048, 1, 1]) in the checkpoint and torch.Size([512, 512, 3, 3]) in the model instantiated\n",
      "- model.backbone.conv_encoder.model.layer4.2.conv1.weight: found shape torch.Size([512, 2048, 1, 1]) in the checkpoint and torch.Size([512, 512, 3, 3]) in the model instantiated\n",
      "- model.input_proj.0.0.weight: found shape torch.Size([256, 512, 1, 1]) in the checkpoint and torch.Size([256, 128, 1, 1]) in the model instantiated\n",
      "- model.input_proj.1.0.weight: found shape torch.Size([256, 1024, 1, 1]) in the checkpoint and torch.Size([256, 256, 1, 1]) in the model instantiated\n",
      "- model.input_proj.2.0.weight: found shape torch.Size([256, 2048, 1, 1]) in the checkpoint and torch.Size([256, 512, 1, 1]) in the model instantiated\n",
      "- model.input_proj.3.0.weight: found shape torch.Size([256, 2048, 3, 3]) in the checkpoint and torch.Size([256, 512, 3, 3]) in the model instantiated\n",
      "- model.query_position_embeddings.weight: found shape torch.Size([300, 512]) in the checkpoint and torch.Size([100, 512]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DeformableDetrForObjectDetection were not initialized from the model checkpoint at SenseTime/deformable-detr and are newly initialized because the shapes did not match:\n",
      "- class_embed.0.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- class_embed.0.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated\n",
      "- model.query_position_embeddings.weight: found shape torch.Size([300, 512]) in the checkpoint and torch.Size([100, 512]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "hyperparam_list = [\n",
    "    ('D-DETR', 'resnet18', 256, 100, 6),\n",
    "    ('D-DETR', 'resnet26', 256, 100, 6),\n",
    "    ('D-DETR', 'resnet34', 256, 100, 6),\n",
    "    ('D-DETR', 'resnet50', 256, 100, 6),\n",
    "]\n",
    "models = []\n",
    "\n",
    "for hyperparams in hyperparam_list:\n",
    "    detr_factory =  DETRFactory(*hyperparams)\n",
    "    model = detr_factory.new_pretrained_model()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18 -> Model: 23.4 M - Backbone: 11.2 M - Transformer: 12.3 M\n",
      "resnet26 -> Model: 30.4 M - Backbone: 13.9 M - Transformer: 16.5 M\n",
      "resnet34 -> Model: 33.5 M - Backbone: 21.3 M - Transformer: 12.3 M\n",
      "resnet50 -> Model: 39.9 M - Backbone: 23.5 M - Transformer: 16.5 M\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cspresnet50.ra_in1k',\n",
       " 'eca_resnet33ts.ra2_in1k',\n",
       " 'ecaresnet26t.ra2_in1k',\n",
       " 'ecaresnet50d.miil_in1k',\n",
       " 'ecaresnet50d_pruned.miil_in1k',\n",
       " 'ecaresnet50t.a1_in1k',\n",
       " 'ecaresnet50t.a2_in1k',\n",
       " 'ecaresnet50t.a3_in1k',\n",
       " 'ecaresnet50t.ra2_in1k',\n",
       " 'ecaresnet101d.miil_in1k',\n",
       " 'ecaresnet101d_pruned.miil_in1k',\n",
       " 'ecaresnet269d.ra2_in1k',\n",
       " 'ecaresnetlight.miil_in1k',\n",
       " 'gcresnet33ts.ra2_in1k',\n",
       " 'gcresnet50t.ra2_in1k',\n",
       " 'inception_resnet_v2.tf_ens_adv_in1k',\n",
       " 'inception_resnet_v2.tf_in1k',\n",
       " 'lambda_resnet26rpt_256.c1_in1k',\n",
       " 'lambda_resnet26t.c1_in1k',\n",
       " 'lambda_resnet50ts.a1h_in1k',\n",
       " 'legacy_seresnet18.in1k',\n",
       " 'legacy_seresnet34.in1k',\n",
       " 'legacy_seresnet50.in1k',\n",
       " 'legacy_seresnet101.in1k',\n",
       " 'legacy_seresnet152.in1k',\n",
       " 'nf_resnet50.ra2_in1k',\n",
       " 'resnet10t.c3_in1k',\n",
       " 'resnet14t.c3_in1k',\n",
       " 'resnet18.a1_in1k',\n",
       " 'resnet18.a2_in1k',\n",
       " 'resnet18.a3_in1k',\n",
       " 'resnet18.fb_ssl_yfcc100m_ft_in1k',\n",
       " 'resnet18.fb_swsl_ig1b_ft_in1k',\n",
       " 'resnet18.gluon_in1k',\n",
       " 'resnet18.tv_in1k',\n",
       " 'resnet18d.ra2_in1k',\n",
       " 'resnet26.bt_in1k',\n",
       " 'resnet26d.bt_in1k',\n",
       " 'resnet26t.ra2_in1k',\n",
       " 'resnet32ts.ra2_in1k',\n",
       " 'resnet33ts.ra2_in1k',\n",
       " 'resnet34.a1_in1k',\n",
       " 'resnet34.a2_in1k',\n",
       " 'resnet34.a3_in1k',\n",
       " 'resnet34.bt_in1k',\n",
       " 'resnet34.gluon_in1k',\n",
       " 'resnet34.tv_in1k',\n",
       " 'resnet34d.ra2_in1k',\n",
       " 'resnet50.a1_in1k',\n",
       " 'resnet50.a1h_in1k',\n",
       " 'resnet50.a2_in1k',\n",
       " 'resnet50.a3_in1k',\n",
       " 'resnet50.am_in1k',\n",
       " 'resnet50.b1k_in1k',\n",
       " 'resnet50.b2k_in1k',\n",
       " 'resnet50.bt_in1k',\n",
       " 'resnet50.c1_in1k',\n",
       " 'resnet50.c2_in1k',\n",
       " 'resnet50.d_in1k',\n",
       " 'resnet50.fb_ssl_yfcc100m_ft_in1k',\n",
       " 'resnet50.fb_swsl_ig1b_ft_in1k',\n",
       " 'resnet50.gluon_in1k',\n",
       " 'resnet50.ra_in1k',\n",
       " 'resnet50.ram_in1k',\n",
       " 'resnet50.tv2_in1k',\n",
       " 'resnet50.tv_in1k',\n",
       " 'resnet50_gn.a1h_in1k',\n",
       " 'resnet50c.gluon_in1k',\n",
       " 'resnet50d.a1_in1k',\n",
       " 'resnet50d.a2_in1k',\n",
       " 'resnet50d.a3_in1k',\n",
       " 'resnet50d.gluon_in1k',\n",
       " 'resnet50d.ra2_in1k',\n",
       " 'resnet50s.gluon_in1k',\n",
       " 'resnet51q.ra2_in1k',\n",
       " 'resnet61q.ra2_in1k',\n",
       " 'resnet101.a1_in1k',\n",
       " 'resnet101.a1h_in1k',\n",
       " 'resnet101.a2_in1k',\n",
       " 'resnet101.a3_in1k',\n",
       " 'resnet101.gluon_in1k',\n",
       " 'resnet101.tv2_in1k',\n",
       " 'resnet101.tv_in1k',\n",
       " 'resnet101c.gluon_in1k',\n",
       " 'resnet101d.gluon_in1k',\n",
       " 'resnet101d.ra2_in1k',\n",
       " 'resnet101s.gluon_in1k',\n",
       " 'resnet152.a1_in1k',\n",
       " 'resnet152.a1h_in1k',\n",
       " 'resnet152.a2_in1k',\n",
       " 'resnet152.a3_in1k',\n",
       " 'resnet152.gluon_in1k',\n",
       " 'resnet152.tv2_in1k',\n",
       " 'resnet152.tv_in1k',\n",
       " 'resnet152c.gluon_in1k',\n",
       " 'resnet152d.gluon_in1k',\n",
       " 'resnet152d.ra2_in1k',\n",
       " 'resnet152s.gluon_in1k',\n",
       " 'resnet200d.ra2_in1k',\n",
       " 'resnetaa50.a1h_in1k',\n",
       " 'resnetaa50d.d_in12k',\n",
       " 'resnetaa50d.sw_in12k',\n",
       " 'resnetaa50d.sw_in12k_ft_in1k',\n",
       " 'resnetaa101d.sw_in12k',\n",
       " 'resnetaa101d.sw_in12k_ft_in1k',\n",
       " 'resnetblur50.bt_in1k',\n",
       " 'resnetrs50.tf_in1k',\n",
       " 'resnetrs101.tf_in1k',\n",
       " 'resnetrs152.tf_in1k',\n",
       " 'resnetrs200.tf_in1k',\n",
       " 'resnetrs270.tf_in1k',\n",
       " 'resnetrs350.tf_in1k',\n",
       " 'resnetrs420.tf_in1k',\n",
       " 'resnetv2_50.a1h_in1k',\n",
       " 'resnetv2_50d_evos.ah_in1k',\n",
       " 'resnetv2_50d_gn.ah_in1k',\n",
       " 'resnetv2_50x1_bit.goog_distilled_in1k',\n",
       " 'resnetv2_50x1_bit.goog_in21k',\n",
       " 'resnetv2_50x1_bit.goog_in21k_ft_in1k',\n",
       " 'resnetv2_50x3_bit.goog_in21k',\n",
       " 'resnetv2_50x3_bit.goog_in21k_ft_in1k',\n",
       " 'resnetv2_101.a1h_in1k',\n",
       " 'resnetv2_101x1_bit.goog_in21k',\n",
       " 'resnetv2_101x1_bit.goog_in21k_ft_in1k',\n",
       " 'resnetv2_101x3_bit.goog_in21k',\n",
       " 'resnetv2_101x3_bit.goog_in21k_ft_in1k',\n",
       " 'resnetv2_152x2_bit.goog_in21k',\n",
       " 'resnetv2_152x2_bit.goog_in21k_ft_in1k',\n",
       " 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k',\n",
       " 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384',\n",
       " 'resnetv2_152x4_bit.goog_in21k',\n",
       " 'resnetv2_152x4_bit.goog_in21k_ft_in1k',\n",
       " 'seresnet33ts.ra2_in1k',\n",
       " 'seresnet50.a1_in1k',\n",
       " 'seresnet50.a2_in1k',\n",
       " 'seresnet50.a3_in1k',\n",
       " 'seresnet50.ra2_in1k',\n",
       " 'seresnet152d.ra2_in1k',\n",
       " 'skresnet18.ra_in1k',\n",
       " 'skresnet34.ra_in1k',\n",
       " 'tresnet_l.miil_in1k',\n",
       " 'tresnet_l.miil_in1k_448',\n",
       " 'tresnet_m.miil_in1k',\n",
       " 'tresnet_m.miil_in1k_448',\n",
       " 'tresnet_m.miil_in21k',\n",
       " 'tresnet_m.miil_in21k_ft_in1k',\n",
       " 'tresnet_v2_l.miil_in21k',\n",
       " 'tresnet_v2_l.miil_in21k_ft_in1k',\n",
       " 'tresnet_xl.miil_in1k',\n",
       " 'tresnet_xl.miil_in1k_448',\n",
       " 'wide_resnet50_2.racm_in1k',\n",
       " 'wide_resnet50_2.tv2_in1k',\n",
       " 'wide_resnet50_2.tv_in1k',\n",
       " 'wide_resnet101_2.tv2_in1k',\n",
       " 'wide_resnet101_2.tv_in1k']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm_models = timm.list_models(pretrained=True)\n",
    "resnet_models = [m for m in timm_models if 'resnet' in m]\n",
    "resnet_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detr-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
