{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Transforms v2: End-to-end object detection example\n",
        "\n",
        "Object detection is not supported out of the box by ``torchvision.transforms`` v1, since it only supports images.\n",
        "``torchvision.transforms.v2`` enables jointly transforming images, videos, bounding boxes, and masks. This example\n",
        "showcases an end-to-end object detection training using the stable ``torchvisio.datasets`` and ``torchvision.models`` as\n",
        "well as the new ``torchvision.transforms.v2`` v2 API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "from collections import defaultdict\n",
        "\n",
        "import PIL.Image\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "import torchvision\n",
        "\n",
        "\n",
        "def show(sample):\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    from torchvision.transforms.v2 import functional as F\n",
        "    from torchvision.utils import draw_bounding_boxes\n",
        "\n",
        "    image, target = sample\n",
        "    if isinstance(image, PIL.Image.Image):\n",
        "        image = F.to_image_tensor(image)\n",
        "    image = F.convert_dtype(image, torch.uint8)\n",
        "    annotated_image = draw_bounding_boxes(image, target[\"boxes\"], colors=\"yellow\", width=3)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(annotated_image.permute(1, 2, 0).numpy())\n",
        "    ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "    fig.tight_layout()\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "# We are using BETA APIs, so we deactivate the associated warning, thereby acknowledging that\n",
        "# some APIs may slightly change in the future\n",
        "torchvision.disable_beta_transforms_warning()\n",
        "\n",
        "from torchvision import models, datasets\n",
        "import torchvision.transforms.v2 as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start off by loading the :class:`~torchvision.datasets.CocoDetection` dataset to have a look at what it currently\n",
        "returns, and we'll see how to convert it to a format that is compatible with our new transforms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "<class 'PIL.Image.Image'>\n",
            "<class 'list'> <class 'dict'> ['id', 'image_id', 'category_id', 'bbox', 'area', 'segmentation', 'iscrowd']\n"
          ]
        }
      ],
      "source": [
        "def load_example_coco_detection_dataset(**kwargs):\n",
        "    # This loads fake data for illustration purposes of this example. In practice, you'll have\n",
        "    # to replace this with the proper data\n",
        "    root = './InBreast_Coco/test'\n",
        "    return datasets.CocoDetection(root, f'{root}/_annotations.coco.json', **kwargs)\n",
        "\n",
        "\n",
        "dataset = load_example_coco_detection_dataset()\n",
        "\n",
        "sample = dataset[0]\n",
        "image, target = sample\n",
        "print(type(image))\n",
        "print(type(target), type(target[0]), list(target[0].keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset returns a two-tuple with the first item being a :class:`PIL.Image.Image` and second one a list of\n",
        "dictionaries, which each containing the annotations for a single object instance. As is, this format is not compatible\n",
        "with the ``torchvision.transforms.v2``, nor with the models. To overcome that, we provide the\n",
        ":func:`~torchvision.datasets.wrap_dataset_for_transforms_v2` function. For\n",
        ":class:`~torchvision.datasets.CocoDetection`, this changes the target structure to a single dictionary of lists. It\n",
        "also adds the key-value-pairs ``\"boxes\"``, ``\"masks\"``, and ``\"labels\"`` wrapped in the corresponding\n",
        "``torchvision.datapoints``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Alejandro Duque\\Documents\\USFQ\\Proyecto Integrador\\Workspace\\DETR\\plot_transforms_v2_e2e.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alejandro%20Duque/Documents/USFQ/Proyecto%20Integrador/Workspace/DETR/plot_transforms_v2_e2e.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mwrap_dataset_for_transforms_v2(dataset)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Alejandro%20Duque/Documents/USFQ/Proyecto%20Integrador/Workspace/DETR/plot_transforms_v2_e2e.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m sample \u001b[39m=\u001b[39m dataset[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alejandro%20Duque/Documents/USFQ/Proyecto%20Integrador/Workspace/DETR/plot_transforms_v2_e2e.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m image, target \u001b[39m=\u001b[39m sample\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alejandro%20Duque/Documents/USFQ/Proyecto%20Integrador/Workspace/DETR/plot_transforms_v2_e2e.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(image))\n",
            "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\detr-env\\Lib\\site-packages\\torchvision\\datapoints\\_dataset_wrapper.py:149\u001b[0m, in \u001b[0;36mVisionDatasetDatapointWrapper.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m    145\u001b[0m     \u001b[39m# This gets us the raw sample since we disabled the transforms for the underlying dataset in the constructor\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[39m# of this class\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset[idx]\n\u001b[1;32m--> 149\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapper(idx, sample)\n\u001b[0;32m    151\u001b[0m     \u001b[39m# Regardless of whether the user has supplied the transforms individually (`transform` and `target_transform`)\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[39m# or joint (`transforms`), we can access the full functionality through `transforms`\u001b[39;00m\n\u001b[0;32m    153\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\detr-env\\Lib\\site-packages\\torchvision\\datapoints\\_dataset_wrapper.py:306\u001b[0m, in \u001b[0;36mcoco_dectection_wrapper_factory.<locals>.wrapper\u001b[1;34m(idx, sample)\u001b[0m\n\u001b[0;32m    295\u001b[0m spatial_size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(F\u001b[39m.\u001b[39mget_spatial_size(image))\n\u001b[0;32m    296\u001b[0m batched_target[\u001b[39m\"\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mconvert_format_bounding_box(\n\u001b[0;32m    297\u001b[0m     datapoints\u001b[39m.\u001b[39mBoundingBox(\n\u001b[0;32m    298\u001b[0m         batched_target[\u001b[39m\"\u001b[39m\u001b[39mbbox\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    302\u001b[0m     new_format\u001b[39m=\u001b[39mdatapoints\u001b[39m.\u001b[39mBoundingBoxFormat\u001b[39m.\u001b[39mXYXY,\n\u001b[0;32m    303\u001b[0m )\n\u001b[0;32m    304\u001b[0m batched_target[\u001b[39m\"\u001b[39m\u001b[39mmasks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m datapoints\u001b[39m.\u001b[39mMask(\n\u001b[0;32m    305\u001b[0m     torch\u001b[39m.\u001b[39mstack(\n\u001b[1;32m--> 306\u001b[0m         [\n\u001b[0;32m    307\u001b[0m             segmentation_to_mask(segmentation, spatial_size\u001b[39m=\u001b[39;49mspatial_size)\n\u001b[0;32m    308\u001b[0m             \u001b[39mfor\u001b[39;49;00m segmentation \u001b[39min\u001b[39;49;00m batched_target[\u001b[39m\"\u001b[39;49m\u001b[39msegmentation\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m    309\u001b[0m         ]\n\u001b[0;32m    310\u001b[0m     ),\n\u001b[0;32m    311\u001b[0m )\n\u001b[0;32m    312\u001b[0m batched_target[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(batched_target[\u001b[39m\"\u001b[39m\u001b[39mcategory_id\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    314\u001b[0m \u001b[39mreturn\u001b[39;00m image, batched_target\n",
            "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\detr-env\\Lib\\site-packages\\torchvision\\datapoints\\_dataset_wrapper.py:307\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    295\u001b[0m spatial_size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(F\u001b[39m.\u001b[39mget_spatial_size(image))\n\u001b[0;32m    296\u001b[0m batched_target[\u001b[39m\"\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mconvert_format_bounding_box(\n\u001b[0;32m    297\u001b[0m     datapoints\u001b[39m.\u001b[39mBoundingBox(\n\u001b[0;32m    298\u001b[0m         batched_target[\u001b[39m\"\u001b[39m\u001b[39mbbox\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    302\u001b[0m     new_format\u001b[39m=\u001b[39mdatapoints\u001b[39m.\u001b[39mBoundingBoxFormat\u001b[39m.\u001b[39mXYXY,\n\u001b[0;32m    303\u001b[0m )\n\u001b[0;32m    304\u001b[0m batched_target[\u001b[39m\"\u001b[39m\u001b[39mmasks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m datapoints\u001b[39m.\u001b[39mMask(\n\u001b[0;32m    305\u001b[0m     torch\u001b[39m.\u001b[39mstack(\n\u001b[0;32m    306\u001b[0m         [\n\u001b[1;32m--> 307\u001b[0m             segmentation_to_mask(segmentation, spatial_size\u001b[39m=\u001b[39;49mspatial_size)\n\u001b[0;32m    308\u001b[0m             \u001b[39mfor\u001b[39;00m segmentation \u001b[39min\u001b[39;00m batched_target[\u001b[39m\"\u001b[39m\u001b[39msegmentation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    309\u001b[0m         ]\n\u001b[0;32m    310\u001b[0m     ),\n\u001b[0;32m    311\u001b[0m )\n\u001b[0;32m    312\u001b[0m batched_target[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(batched_target[\u001b[39m\"\u001b[39m\u001b[39mcategory_id\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    314\u001b[0m \u001b[39mreturn\u001b[39;00m image, batched_target\n",
            "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\detr-env\\Lib\\site-packages\\torchvision\\datapoints\\_dataset_wrapper.py:279\u001b[0m, in \u001b[0;36mcoco_dectection_wrapper_factory.<locals>.segmentation_to_mask\u001b[1;34m(segmentation, spatial_size)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msegmentation_to_mask\u001b[39m(segmentation, \u001b[39m*\u001b[39m, spatial_size):\n\u001b[0;32m    274\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpycocotools\u001b[39;00m \u001b[39mimport\u001b[39;00m mask\n\u001b[0;32m    276\u001b[0m     segmentation \u001b[39m=\u001b[39m (\n\u001b[0;32m    277\u001b[0m         mask\u001b[39m.\u001b[39mfrPyObjects(segmentation, \u001b[39m*\u001b[39mspatial_size)\n\u001b[0;32m    278\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(segmentation, \u001b[39mdict\u001b[39m)\n\u001b[1;32m--> 279\u001b[0m         \u001b[39melse\u001b[39;00m mask\u001b[39m.\u001b[39mmerge(mask\u001b[39m.\u001b[39;49mfrPyObjects(segmentation, \u001b[39m*\u001b[39;49mspatial_size))\n\u001b[0;32m    280\u001b[0m     )\n\u001b[0;32m    281\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mfrom_numpy(mask\u001b[39m.\u001b[39mdecode(segmentation))\n",
            "File \u001b[1;32mpycocotools\\\\_mask.pyx:294\u001b[0m, in \u001b[0;36mpycocotools._mask.frPyObjects\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "dataset = datasets.wrap_dataset_for_transforms_v2(dataset)\n",
        "\n",
        "sample = dataset[0]\n",
        "image, target = sample\n",
        "print(type(image))\n",
        "print(type(target), list(target.keys()))\n",
        "print(type(target[\"boxes\"]), type(target[\"masks\"]), type(target[\"labels\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As baseline, let's have a look at a sample without transformations:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "show(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the dataset properly set up, we can now define the augmentation pipeline. This is done the same way it is done in\n",
        "``torchvision.transforms`` v1, but now handles bounding boxes and masks without any extra configuration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomPhotometricDistort(),\n",
        "        transforms.RandomZoomOut(\n",
        "            fill=defaultdict(lambda: 0, {PIL.Image.Image: (123, 117, 104)})\n",
        "        ),\n",
        "        transforms.RandomIoUCrop(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToImageTensor(),\n",
        "        transforms.ConvertImageDtype(torch.float32),\n",
        "        transforms.SanitizeBoundingBox(),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Although the :class:`~torchvision.transforms.v2.SanitizeBoundingBox` transform is a no-op in this example, but it\n",
        "   should be placed at least once at the end of a detection pipeline to remove degenerate bounding boxes as well as\n",
        "   the corresponding labels and optionally masks. It is particularly critical to add it if\n",
        "   :class:`~torchvision.transforms.v2.RandomIoUCrop` was used.</p></div>\n",
        "\n",
        "Let's look how the sample looks like with our augmentation pipeline in place:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = load_example_coco_detection_dataset(transforms=transform)\n",
        "dataset = datasets.wrap_dataset_for_transforms_v2(dataset)\n",
        "\n",
        "torch.manual_seed(3141)\n",
        "sample = dataset[0]\n",
        "\n",
        "show(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the color of the image was distorted, we zoomed out on it (off center) and flipped it horizontally.\n",
        "In all of this, the bounding box was transformed accordingly. And without any further ado, we can start training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    # We need a custom collation function here, since the object detection models expect a\n",
        "    # sequence of images and target dictionaries. The default collation function tries to\n",
        "    # `torch.stack` the individual elements, which fails in general for object detection,\n",
        "    # because the number of object instances varies between the samples. This is the same for\n",
        "    # `torchvision.transforms` v1\n",
        "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
        ")\n",
        "\n",
        "model = models.get_model(\"ssd300_vgg16\", weights=None, weights_backbone=None).train()\n",
        "\n",
        "for images, targets in data_loader:\n",
        "    loss_dict = model(images, targets)\n",
        "    print(loss_dict)\n",
        "    # Put your training logic here\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
